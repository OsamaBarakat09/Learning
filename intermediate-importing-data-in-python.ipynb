{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "422905ce",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-05T14:24:42.860090Z",
     "iopub.status.busy": "2022-08-05T14:24:42.859450Z",
     "iopub.status.idle": "2022-08-05T14:24:43.947820Z",
     "shell.execute_reply": "2022-08-05T14:24:43.946622Z"
    },
    "papermill": {
     "duration": 1.096585,
     "end_time": "2022-08-05T14:24:43.950802",
     "exception": false,
     "start_time": "2022-08-05T14:24:42.854217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batchcomplete:  \n",
      "warnings:  {'extracts': {'*': 'HTML may be malformed and/or unbalanced and may omit inline images. Use at your own risk. Known problems are listed at https://www.mediawiki.org/wiki/Special:MyLanguage/Extension:TextExtracts#Caveats.'}}\n",
      "query:  {'normalized': [{'from': 'pizz', 'to': 'Pizz'}], 'pages': {'12314082': {'pageid': 12314082, 'ns': 0, 'title': 'Pizz', 'extract': '<!-- \\nNewPP limit report\\nParsed by mw1443\\nCached time: 20220805122611\\nCache expiry: 1814400\\nReduced expiry: false\\nComplications: []\\nCPU time usage: 0.003 seconds\\nReal time usage: 0.004 seconds\\nPreprocessor visited node count: 0/1000000\\nPost‐expand include size: 0/2097152 bytes\\nTemplate argument size: 0/2097152 bytes\\nHighest expansion depth: 0/100\\nExpensive parser function count: 0/500\\nUnstrip recursion depth: 0/20\\nUnstrip post‐expand size: 0/5000000 bytes\\nNumber of Wikibase entities loaded: 0/400\\n-->\\n<!--\\nTransclusion expansion time report (%,ms,calls,template)\\n100.00%    0.000      1 -total\\n-->'}}}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen, Request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "class Importing_Flat_File_From_The_Web():\n",
    "    def locally_use():\n",
    "        # Assign url of file: url\n",
    "        url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "\n",
    "        # Save file locally\n",
    "        urlretrieve(url,'winequality-red.csv')\n",
    "\n",
    "        # Read file into a DataFrame and print its head\n",
    "        df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "        return df.head()\n",
    "\n",
    "\n",
    "    def locally_Notuse():\n",
    "        # Assign url of file: url\n",
    "        url = 'https://assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "        # Read file into a DataFrame: df\n",
    "        df = pd.read_csv(url, sep=';')\n",
    "\n",
    "        # Print the head of the DataFrame\n",
    "        print(df.head())\n",
    "\n",
    "        # Plot first column of df\n",
    "        df.iloc[:, 0].hist()\n",
    "        plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "        plt.ylabel('count')\n",
    "        plt.show()\n",
    "    def locally_Excel():\n",
    "        # Assign url of file: url\n",
    "        url = 'https://assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "        # Read in all sheets of Excel file: \n",
    "        xls_1 = pd.read_excel(url)\n",
    "        xls = pd.read_excel(url, sheet_name=None)\n",
    "        print(xls_1)\n",
    "        print('/////////////////////////////////////////////////////////////////////////')\n",
    "        print(xls)\n",
    "        print('/////////////////////////////////////////////////////////////////////////')\n",
    "        # Print the sheetnames to the shell\n",
    "        print(xls.keys())\n",
    "        print('/////////////////////////////////////////////////////////////////////////')\n",
    "\n",
    "        # Print the head of the first sheet (using its name, NOT its index)\n",
    "        print(xls['1700'].head())\n",
    "class HTTP_Request_To_Import_File_From_The_Web():\n",
    "    def Get_Requestes_Using_Urllib():\n",
    "        url = 'https://pandas.pydata.org/'\n",
    "        request = Request(url)\n",
    "        # This packages the request\n",
    "        response = urlopen(request)\n",
    "        # Sends the request and catches the response\n",
    "        html = response.read()\n",
    "        # Be polite and close the response! ☺☺☺\n",
    "        response.close()\n",
    "        return html\n",
    "    def Get_Requestes_Using_Requist():\n",
    "        # Specify the url\n",
    "        url = 'https://pandas.pydata.org/'\n",
    "        df = requests.get(url)\n",
    "        text = df.textp\n",
    "        return text\n",
    "    \n",
    "class Scarping_The_web_In_Python():\n",
    "    def Beautiful_Soup():\n",
    "        url = 'https://www.crummy.com/software/BeautifulSoup/'\n",
    "        r = requests.get(url)\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc)\n",
    "        return soup\n",
    "        return soup.prettify\n",
    "    def Exploring_Beautiful_Soup():\n",
    "        url = 'https://www.crummy.com/software/BeautifulSoup/'\n",
    "        r = requests.get(url)\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc)\n",
    "        return soup.title\n",
    "        return soup.get_text()\n",
    "    def Exploring_Beautiful_Soup_2():\n",
    "        url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "        # Package the request, send the request and catch the response\n",
    "        r = requests.get(url)\n",
    "        # Extract the response as html\n",
    "        html_doc = r.text\n",
    "        soup = BeautifulSoup(html_doc)\n",
    "        for link in soup.find_all('a'):\n",
    "             print(link.get('href'))\n",
    "    \n",
    "class Interact_With_Apls_To_Import_Data_From_The_Web():\n",
    "    def API_Requests():\n",
    "        # Assign URL to variable: url\n",
    "        url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "        # Package the request, send the request and catch the response: r\n",
    "        r = requests.get(url)\n",
    "\n",
    "        # Print the text of the response\n",
    "        print(r.text)\n",
    "\n",
    "    def Json_From_The_Web_python():\n",
    "    # Assign URL to variable: url\n",
    "        url = 'https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizz'\n",
    "\n",
    "        # Package the request, send the request and catch the response: r\n",
    "        r = requests.get(url)\n",
    "\n",
    "        # Decode the JSON data into a dictionary: json_data\n",
    "        json_data = r.json()\n",
    "\n",
    "        # Print each key-value pair in json_data\n",
    "        for k in json_data.keys():\n",
    "            print(k + ': ', json_data[k])\n",
    "\n",
    "            \n",
    "            \n",
    "Interact_With_Apls_To_Import_Data_From_The_Web.Json_From_The_Web_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945b89c",
   "metadata": {
    "papermill": {
     "duration": 0.001574,
     "end_time": "2022-08-05T14:24:43.954647",
     "exception": false,
     "start_time": "2022-08-05T14:24:43.953073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9858e",
   "metadata": {
    "papermill": {
     "duration": 0.001501,
     "end_time": "2022-08-05T14:24:43.957876",
     "exception": false,
     "start_time": "2022-08-05T14:24:43.956375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.856881,
   "end_time": "2022-08-05T14:24:44.579291",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-05T14:24:33.722410",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
